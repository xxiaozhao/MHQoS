// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
// vim: ts=8 sw=2 smarttab
/*
 * Ceph - scalable distributed file system
 *
 * Copyright (C) 2016 Red Hat Inc.
 *
 * This is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Lesser General Public
 * License version 2.1, as published by the Free Software
 * Foundation.  See file COPYING.
 *
 */


#pragma once

#include <ostream>
#include <map>
#include <vector>

//咯咯哒
#include <unordered_map>
#include <string>
#include <chrono>
#include <iostream>

#include "boost/variant.hpp"

#include "dmclock/src/dmclock_server.h"

#include "osd/scheduler/OpScheduler.h"
#include "common/config.h"
#include "include/cmp.h"
#include "common/ceph_context.h"
#include "common/mClockPriorityQueue.h"
#include "osd/scheduler/OpSchedulerItem.h"



namespace ceph::osd::scheduler {

constexpr double default_min = 0.0;
constexpr double default_max = std::numeric_limits<double>::is_iec559 ?
  std::numeric_limits<double>::infinity() :
  std::numeric_limits<double>::max();

/**
 * client_profile_id_t
 *
 * client_id - global id (client.####) for client QoS
 * profile_id - id generated by client's QoS profile
 *
 * Currently (Reef and below), both members are set to
 * 0 which ensures that all external clients share the
 * mClock profile allocated reservation and limit
 * bandwidth.
 *
 * Note: Post Reef, both members will be set to non-zero
 * values when the distributed feature of the mClock
 * algorithm is utilized.
 */
// 这段注释描述了client_profile_id_t结构体的含义和成员变量的作用：

// client_id：客户端的全局ID，通常采用client.####的格式表示。用于客户端的QoS（Quality of Service）。
// profile_id：由客户端的QoS配置生成的ID。在当前版本（Reef及以下版本）中，这两个成员变量均设置为0，
//以确保所有外部客户端共享mClock配置的预留和限制带宽。在未来版本中，当使用mClock算法的分布式特性时，这两个成员变量将被设置为非零值。

// client_id、profile_id及输出流
struct client_profile_id_t {
  uint64_t client_id = 0;
  uint64_t profile_id = 0;
  // 咯咯哒 用户名
  std::string entityName;

  client_profile_id_t(uint64_t _client_id, uint64_t _profile_id) :
    client_id(_client_id),
    profile_id(_profile_id) {}

  client_profile_id_t() = default;

  client_profile_id_t(std::string entityName) :
    entityName(entityName) {}

  friend std::ostream& operator<<(std::ostream& out,
                                  const client_profile_id_t& client_profile) {
    out << " client_id: " << client_profile.client_id
        << " profile_id: " << client_profile.profile_id
    	<< " entityName: " << client_profile.entityName;
    return out;
  }
};



WRITE_EQ_OPERATORS_3(client_profile_id_t, client_id, profile_id, entityName)
WRITE_CMP_OPERATORS_3(client_profile_id_t, client_id, profile_id, entityName)

//调度器的类别、客户端及配置文件ID
struct scheduler_id_t {
  op_scheduler_class class_id;    //调度器的类别background_recovery、background_best_effort、immediate、 client、zjz、mxqh
  client_profile_id_t client_profile_id;    //客户端、配置文件ID

  friend std::ostream& operator<<(std::ostream& out,
                                  const scheduler_id_t& sched_id) {
    out << "{ class_id: " << sched_id.class_id
        << sched_id.client_profile_id;
    return out << " }";
  }
};

WRITE_EQ_OPERATORS_2(scheduler_id_t, class_id, client_profile_id)
WRITE_CMP_OPERATORS_2(scheduler_id_t, class_id, client_profile_id)

/**
 * Scheduler implementation based on mclock.
 *
 * TODO: explain configs
 */
// md_config_obs_t是另一个基类，它提供了配置观察者的功能。
class mClockScheduler : public OpScheduler, md_config_obs_t {

  CephContext *cct;             //指向CephContext的指针，表示Ceph上下文
  const int whoami;             //表示当前OSD的ID。
  const uint32_t num_shards;    //表示OSD的分片数。
  const int shard_id;           //表示当前OSD的分片ID。
  const bool is_rotational;     //表示当前OSD的存储介质是否是旋转式硬盘。
  MonClient *monc;              //指向MonClient的指针，表示监视器客户端。

  std::string cur_mclock_profile;    //调度器当前模板
  std::string pre_mclock_profile;    //调度器上一个模板



  #ifdef WITH_SEASTAR
        static constexpr int data_mtx = 0;
        struct DataGuard { DataGuard(int) {} };
  #else
        mutable std::mutex data_mtx;
        using DataGuard = std::lock_guard<decltype(data_mtx)>;
  #endif


  using Clock = std::chrono::steady_clock;
  using TimePoint = Clock::time_point;
  using Duration = std::chrono::milliseconds;

// performance data collection
//    int client_count = 0;   // 当前用户数量
    int idle_count = 0;     // 空闲用户数量
    int erased_count = 0;   // 清理用户数量

    // 当前用户数量
    struct Client_count {
    int high;
    int client;
    int low;

    Client_count() : high(0), client(0), low(0) {}

    int getTotalCount() const {
        return high + client + low;
        }

    }client_count;

//用于存储每个类型的负载和权重
	typedef struct LoadWeight{
    		double load;
    		double wgt;

		// 结构体的构造函数
    		LoadWeight(double l, double w) : load(l), wgt(w) {}
	} LoadWeight;

      //单位时间内各类型进入dmclock队列的数量
  struct Request_count {
    struct type {
        int current_count = 0;
        int last_count = 0;
    };

    struct type high;
    struct type medium;
    struct type low;
    struct type background_recovery;
    struct type background_medium;
    struct type background_best_effort;

    TimePoint last_time;

    // 默认构造函数
    Request_count() : last_time(Clock::now()) {}

    void add(const std::string& type, mClockScheduler* scheduler) {
        TimePoint current = Clock::now();
        if (std::chrono::duration_cast<std::chrono::seconds>(current - last_time).count() > 1) { // 时间间隔可以自定义
            last_time = current;
            high.last_count = high.current_count;
            high.current_count = 0;
            medium.last_count = medium.current_count;
            medium.current_count = 0;
            low.last_count = low.current_count;
            low.current_count = 0;
            background_recovery.last_count = background_recovery.current_count;
            background_recovery.current_count = 0;
            background_medium.last_count = background_medium.current_count;
            background_medium.current_count = 0;
            background_best_effort.last_count = background_best_effort.current_count;
            background_best_effort.current_count = 0;

          //更新模板参数
           scheduler->dynamic_adjustment_config();

        }

        if (type == "high") {
            high.current_count++;
        } else if (type == "medium") {
            medium.current_count++;
        } else if (type == "low") {
            low.current_count++;
        } else if (type == "background_recovery") {
            background_recovery.current_count++;
        } else if (type == "background_medium") {
            background_medium.current_count++;
        } else if (type == "background_best_effort") {
            background_best_effort.current_count++;
        }
    }

    int getTotalCount() const {
        return high.last_count + medium.last_count + low.last_count + background_recovery.last_count + background_medium.last_count + background_best_effort.last_count;
    }

} request_count;


    Duration                idle_age  = std::chrono::seconds(300);
    Duration                erase_age = std::chrono::seconds(600);
//  double                  check_time = 60;
    Duration                init_check_time = std::chrono::seconds(1);     //初始化检测间隔
    double                  check_time = 60;             //当前检测间隔（秒）
    double                  fastest_check_time = 5;      //最快检测的时间间隔（秒）
    double                  slowest_check_time = 600;    //最慢检测的时间间隔（秒）
//    double                  aggressive_check_time = 1;   //固定时间间隔加速（秒）
    Duration                aggressive_check_time = std::chrono::seconds(1);   //固定时间间隔加速





    // max number of clients to erase at a time
    int erase_max = 2000;


    // NB: All threads declared at end, so they're destructed first!

    std::unique_ptr<crimson::RunEvery> cleaning_job;



  static std::unordered_map<std::string, int> user_priorities; //用户优先级


    struct UserInfo {
    std::string username;
    int priority;
    TimePoint last_access_time; // 使用 steady_clock::time_point

    // 默认构造函数
    UserInfo() = default;

    // 构造函数
    UserInfo(const std::string& name, int prio)
        : username(name), priority(prio), last_access_time(Clock::now()) {} // 使用 steady_clock::now()
};

// 当前访问该osd切片的所有用户
std::unordered_map<std::string, UserInfo> current_users;


  /**
   * osd_bandwidth_cost_per_io
   *
   * mClock expects all queued items to have a uniform expression of  "cost".  
   * However, IO devices generally have quite different capacity for sequential IO vs small random IO.  
   * This implementation handles this by expressing all costs as a number of sequential bytes written adding additional cost for each random IO equal to osd_bandwidth_cost_per_io.
   *
   * Thus, an IO operation requiring a total of <size> bytes to be written accross <iops> different locations will have a cost of <size> + (osd_bandwidth_cost_per_io * <iops>) bytes.
   *
   * Set in set_osd_capacity_params_from_config in the constructor and upon config change.
   *
   * Has units bytes/io.
   */
  double osd_bandwidth_cost_per_io;     //表示每个IO操作的额外带宽成本，相对于固定cost额外的成本

  /**
   * osd_bandwidth_capacity_per_shard
   *
   * mClock expects reservation and limit paramters to be expressed in units of cost/second -- which means bytes/second for this implementation.
   *
   * Rather than expecting users to compute appropriate limit and reservation values for each class of OSDs in their cluster, 
   * we instead express reservation and limit paramaters as ratios of the OSD's maxmimum capacity.
   * osd_bandwidth_capacity_per_shard is that capacity divided by the number of shards.
   *
   * Set in set_osd_capacity_params_from_config in the constructor and upon config change.
   *
   * This value gets passed to ClientRegistry::update_from_config in order to resolve the full reservaiton and limit parameters for mclock from the configured ratios.
   *
   * Has units bytes/second.
   */

  // 我们将保留和限制参数表示为 OSD 最大容量的比率。
  //osd_bandwidth_capacity_per_shard 是容量除以分片数量。
  double osd_bandwidth_capacity_per_shard;      //表示每个分片的带宽容量。



 // 客户端注册表，用于管理不同类型IO的客户端信息。
  class ClientRegistry {


    // 存储着内部客户端信息——————延迟处理
    std::array<
      crimson::dmclock::ClientInfo,
      static_cast<size_t>(op_scheduler_class::immediate)
    > internal_client_infos = {
      // Placeholder, gets replaced with configured values
      crimson::dmclock::ClientInfo(1, 1, 1, 0),
      crimson::dmclock::ClientInfo(1, 1, 1, 0),        //为background_medium添加占位符
      crimson::dmclock::ClientInfo(1, 1, 1, 0)
    };

    // 是一个默认的外部客户端信息对象
    crimson::dmclock::ClientInfo default_external_client_info = {1, 1, 1, 0};
    crimson::dmclock::ClientInfo default_external_high_info = {1, 1, 1, 0};
    crimson::dmclock::ClientInfo default_external_low_info = {1, 1, 1, 0};   

    
    // 是一个映射，用于存储外部客户端信息
    std::map<client_profile_id_t,
	     crimson::dmclock::ClientInfo> external_client_infos;


    const crimson::dmclock::ClientInfo *get_external_client(
      const client_profile_id_t &client) const;
  public:
    /**
     * update_from_config
     *
     * Sets the mclock paramaters (reservation, weight, and limit)
     * for each class of IO (background_recovery, background_best_effort,
     * and client).
     */
    // 更新的 mclock 参数（预留量、权重和限制）
    void update_from_config(
      const ConfigProxy &conf,
      double capacity_per_shard,
      uint32_t num_shards);

    // 只更新权重参数
    void update_wgt(uint64_t high, uint64_t client, uint64_t low, uint64_t background_recovery, uint64_t background_medium, uint64_t background_best_effort);

    
    // 根据给定的调度器 ID，返回对应的客户端信息对象。
    const crimson::dmclock::ClientInfo *get_info(
      const scheduler_id_t &id) const;
  } client_registry;



// TimePoint last_frozen_time;         //上次冻结时间



  // 基于mclock的调度队列，用于按优先级调度操作。
  using mclock_queue_t = crimson::dmclock::PullPriorityQueue<
    scheduler_id_t,
    OpSchedulerItem,
    true,
    true,
    2>;
  mclock_queue_t scheduler;

  // 立即执行的操作列表。
  std::list<OpSchedulerItem> immediate;

  // 将 OpSchedulerItem 中的调度器类别提取出来，以便后续的调度器操作可以使用。
  static scheduler_id_t get_scheduler_id(const OpSchedulerItem &item) {
    return scheduler_id_t{
      item.get_scheduler_class(),
      client_profile_id_t(item.get_entityName())
    };
  }

  /**
   * set_osd_capacity_params_from_config
   *
   * mClockScheduler uses two parameters, osd_bandwidth_cost_per_io and osd_bandwidth_capacity_per_shard, internally.  
   * These two parameters are derived from config parameters osd_mclock_max_capacity_iops_(hdd|ssd) and osd_mclock_max_sequential_bandwidth_(hdd|ssd) as well as num_shards.
   * Invoking set_osd_capacity_params_from_config() resets those derived params based on the current config and should be invoked any time they are modified as well as in the constructor.  See handle_conf_change().
   */
  // 根据配置重新计算容量、带宽等参数
  void set_osd_capacity_params_from_config();

  // Set the mclock related config params based on the profile
  //根据配置文件设置 mclock 相关配置参数
  void set_config_defaults_from_profile();

  //动态调整模板参数
  void dynamic_adjustment_config();

public:
  mClockScheduler(CephContext *cct, int whoami, uint32_t num_shards,
    int shard_id, bool is_rotational, MonClient *monc);
  ~mClockScheduler() override;

public:
  // 公共的静态成员函数，用于查询特定用户的优先级
    static int get_user_priority(const std::string& username) {
        auto it = user_priorities.find(username);
        if (it != user_priorities.end()) {
            return it->second;
        }
        // 如果找不到用户，则返回默认值或其他指定的值
        return -1; // 例如，这里返回 -1 表示未找到用户
    }


  // 设置用户优先级
    void set_user_priority(const std::string& username, int priority);

  // 获取用户优先级
//    int get_user_priority(const std::string& username) const;
    
  //初始化用户优先级
  static void initUserPriorities() {
          // 为模拟填充用户优先级
          for (int i = 1; i <= 100; ++i) {
              user_priorities["client.high_" + std::to_string(i)] = 1;
              user_priorities["client.medium_" + std::to_string(i)] = 2;
	      user_priorities["client.low_" + std::to_string(i)] = 3;
          }
      }



  //咯咯哒
  void do_clean();



  /// Calculate scaled cost per item
  // 计算每个项目的缩放成本
  uint32_t calc_scaled_cost(int cost);

  // Helper method to display mclock queues
  // 显示mclock队列的辅助方法
  std::string display_queues() const;

  // Enqueue op in the back of the regular queue
  // 将操作排入常规队列的后端。
  void enqueue(OpSchedulerItem &&item) final;

  // Enqueue the op in the front of the regular queue
  // 将操作排入常规队列的前端。
  void enqueue_front(OpSchedulerItem &&item) final;

  // Return an op to be dispatch
  // 返回要调度的操作。
  WorkItem dequeue() final;

  // Returns if the queue is empty
  // 检查队列是否为空。
  bool empty() const final {
    return immediate.empty() && scheduler.empty();
  }

  // Formatted output of the queue
  // 以格式化的方式输出队列内容。
  void dump(ceph::Formatter &f) const final;

  // 打印对象的简要描述。
  void print(std::ostream &ostream) const final {
    ostream << "mClockScheduler";
  }

  // Update data associated with the modified mclock config key(s)
  // 更新与修改mclock配置键相关的数据。
  void update_configuration() final;

  // 获取对象所关注的配置键。
  const char** get_tracked_conf_keys() const final;

  // 处理配置更改
  void handle_conf_change(const ConfigProxy& conf,
			  const std::set<std::string> &changed) final;
};

}
